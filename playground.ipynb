{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Read train data from parquet file in data/train-00000-of-00001-be16864a4346f8b0.parquet\n",
    "train = pd.read_parquet('data/train-00000-of-00001-be16864a4346f8b0.parquet')\n",
    "\n",
    "# Read test data from parquet file in data/test-00000-of-00001-8026e2bb5cef708b.parquet\n",
    "test = pd.read_parquet('data/test-00000-of-00001-8026e2bb5cef708b.parquet')\n",
    "\n",
    "# Read validation data from parquet file in data/validation-00000-of-00001-6242383510343be0.parquet\n",
    "validation = pd.read_parquet('data/validation-00000-of-00001-6242383510343be0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(model_size):\n",
    "    if model_size in [\"small\", \"large\", \"base\", \"xl\", \"xxl\"]:\n",
    "        model = T5ForConditionalGeneration.from_pretrained(f\"google/flan-t5-{model_size}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"google/flan-t5-{model_size}\")\n",
    "    elif model_size == \"eightbitmodel\":\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\", load_in_8bit=True)\n",
    "        model.to(\"mps\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\").input_ids.to(\"mps\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model : {model_size}\")\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, df, name):\n",
    "    print(\"Evaluating model: \", name)\n",
    "    answers = []\n",
    "    score = 0\n",
    "    results = []\n",
    "    for i in df['text']:\n",
    "        inputs = tokenizer(i, return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "        answers.append(tokenizer.decode(outputs[0]))\n",
    "    \n",
    "    for i in range(len(answers)):\n",
    "        x = answers[i].replace('<pad>', '').replace('</s>', '')\n",
    "        a = x.strip()\n",
    "        # append the question, answer, and correct answer to the results list as a dictionary\n",
    "        results.append({'question': df['text'][i], 'answer': a, 'correct_answer': df['answer'][i]})\n",
    "        if a == df['answer'][i]:\n",
    "            score += 1\n",
    "    \n",
    "    print(\"Score: \", score/len(answers))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    model_size = str(input(\"Enter model size: \"))\n",
    "    model, tokenizer = get_model_and_tokenizer(model_size)\n",
    "    results = evaluate(model, tokenizer, train.sample(n=25, random_state=42).reset_index(drop=True), model_size)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the results list to a dataframe\n",
    "results = pd.DataFrame(base)\n",
    "results.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.save_csv(results, 'small-SAT-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('results/small-SAT-results.csv')\n",
    "x.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"hendrycks_test\", 'global_facts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['auxiliary_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['auxiliary_train']['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['auxiliary_train']['choices'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['auxiliary_train']['answer'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Davis decided to kill Adams. He set out for Adams's house. Before he got there he saw Brooks, who resembled Adams. Thinking that Brooks was Adams, Davis shot at Brooks. The shot missed Brooks but wounded Case, who was some distance away. Davis had not seen Case. In a prosecution under a statute that proscribes any attempt to commit murder, the district attorney should indicate that the intended victim(s) was/were\\nA: Adams only., B: Brooks only., C: Case only., D: Adams and Brooks\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset['auxiliary_train'][0]\n",
    "prompt = dataset['question'] + \"\\n\" + \"A: \" + dataset['choices'][0] + \", B: \" + dataset['choices'][1] + \", C: \" + dataset['choices'][2] + \", D: \" + dataset['choices'][3]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mmlu(dataset, model, tokenizer, name):\n",
    "    print(\"Evaluating model: \", name)\n",
    "    answers = []\n",
    "    score = 0\n",
    "    errors = []\n",
    "    results = []\n",
    "    for i in range(len(dataset['question'])):\n",
    "\n",
    "        # The choices should be in the format of \"A: choice1, B: choice2, C: choice3, D: choice4\" and be seperated from the question by a tab\n",
    "        prompt = dataset['question'][i] + \"\\n\" + \"A: \" + dataset['choices'][i][0] + \", B: \" + dataset['choices'][i][1] + \", C: \" + dataset['choices'][i][2] + \", D: \" + dataset['choices'][i][3]\n",
    "        # print(prompt)\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "        a = tokenizer.decode(outputs[0])\n",
    "    \n",
    "        x = a.replace('<pad>', '').replace('</s>', '')\n",
    "        z = x.strip()\n",
    "\n",
    "        answers.append(z)\n",
    "        keys = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "        # append the question, answer, and correct answer to the results list as a dictionary\n",
    "        results.append({'question': dataset['question'][i], 'answer': z, 'correct_answer': list(keys.keys())[list(keys.values()).index(dataset['answer'][i])]})\n",
    "        # print(keys[str(z)])\n",
    "        if keys[str(z)] == dataset['answer'][i]:\n",
    "            score += 1\n",
    "        elif str(z) not in keys.keys():\n",
    "            print(\"Error: \", z)\n",
    "        else:\n",
    "            errors.append({'question': dataset['question'][i], 'answer': z, 'correct_answer': list(keys.keys())[list(keys.values()).index(dataset['answer'][i])]})  \n",
    "    \n",
    "    print(\"Score: \", score/len(answers))\n",
    "    return results, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"One line Code Key value: \", list(keys.keys())[list(keys.values()).index(dataset['answer'][i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mmlu(model_size=\"base\", n=100):\n",
    "    # model_size = str(input(\"Enter model size: \"))\n",
    "    model, tokenizer = get_model_and_tokenizer(model_size)\n",
    "    g = dataset.sample(n=n, random_state=42).reset_index(drop=True)\n",
    "    results, errors = evaluate_mmlu(g, model, tokenizer, model_size)\n",
    "    return results, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model:  base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.32\n"
     ]
    }
   ],
   "source": [
    "results, errors = run_mmlu('base', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>correct_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Davis decided to kill Adams. He set out for Ad...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A state statute requires any person licensed t...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lender met Borrower on the street, demanded th...</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peter sued Don for breach of contract. The cou...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ames had painted Bell's house under a contract...</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ames had painted Bell's house under a contract...</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ames had painted Bell's house under a contract...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The State of Aurora requires licenses of perso...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The State of Aurora requires licenses of perso...</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The State of Aurora requires licenses of perso...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question answer correct_answer\n",
       "0  Davis decided to kill Adams. He set out for Ad...      B              B\n",
       "1  A state statute requires any person licensed t...      D              D\n",
       "2  Lender met Borrower on the street, demanded th...      C              C\n",
       "3  Peter sued Don for breach of contract. The cou...      B              B\n",
       "4  Ames had painted Bell's house under a contract...      B              C\n",
       "5  Ames had painted Bell's house under a contract...      A              C\n",
       "6  Ames had painted Bell's house under a contract...      A              A\n",
       "7  The State of Aurora requires licenses of perso...      A              A\n",
       "8  The State of Aurora requires licenses of perso...      B              D\n",
       "9  The State of Aurora requires licenses of perso...      D              D"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the results list to a dataframe\n",
    "results = pd.DataFrame(results)\n",
    "results.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>correct_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ames had painted Bell's house under a contract...</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ames had painted Bell's house under a contract...</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The State of Aurora requires licenses of perso...</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question answer correct_answer\n",
       "0  Ames had painted Bell's house under a contract...      B              C\n",
       "1  Ames had painted Bell's house under a contract...      A              C\n",
       "2  The State of Aurora requires licenses of perso...      B              D"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the errors list to a dataframe\n",
    "errors = pd.DataFrame(errors)\n",
    "errors.head(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33c24971b795bfb5d78126ed55bc3a1b870bcf64fc6ad2b69905801854680e56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
